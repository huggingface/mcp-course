# Tiny Agents: 一个由MCP驱动的50行代码的智能体

既然我们已经用Gradio构建了MCP服务器，让我们进一步探索MCP客户端。本节基于实验项目[Tiny Agents](https://huggingface.co/blog/tiny-agents)，该项目展示了一种超级简单的方式来部署MCP客户端，这些客户端可以连接到像我们之前构建的Gradio情感分析服务器这样的服务。

在这个简短的练习中，我们将指导你如何实现一个TypeScript (JS) MCP客户端，它可以与任何MCP服务器通信，包括我们在上一节中构建的基于Gradio的情感分析服务器。你将看到MCP如何标准化智能体与工具的交互方式，使Agentic AI开发变得显著简单。

![meme](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/tiny-agents/thumbnail.jpg)
<figcaption>图片来源 https://x.com/adamdotdev</figcaption>

我们将向你展示如何将你的tiny agent连接到基于Gradio的MCP服务器，使其能够利用你的自定义情感分析工具和其他预构建工具。

## 如何运行完整演示

如果你有NodeJS（带有`pnpm`或`npm`），只需在终端中运行：

```bash
npx @huggingface/mcp-client
```

或者如果使用`pnpm`：

```bash
pnpx @huggingface/mcp-client
```

这会将包安装到临时文件夹中，然后执行其命令。

你将看到你的简单Agent连接到多个MCP服务器（在本地运行），加载它们的工具（类似于它如何加载你的Gradio情感分析工具），然后提示你进行对话。

<video controls autoplay loop>
  <source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/tiny-agents/use-filesystem.mp4" type="video/mp4">
</video>

默认情况下，我们的示例Agent连接到以下两个MCP服务器：

- "规范"的[文件系统服务器](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem)，它可以访问你的桌面，
- 以及[Playwright MCP](https://github.com/microsoft/playwright-mcp)服务器，它知道如何为你使用沙盒化的Chromium浏览器。

你可以轻松地将你的Gradio情感分析服务器添加到此列表中，我们将在本节后面演示。

> [!NOTE]
> 注意：这有点反直觉，但目前tiny agents中的所有MCP服务器实际上都是本地进程（尽管远程服务器即将推出）。这不包括我们在localhost:7860上运行的Gradio服务器。

我们第一个视频的输入是：

> 写一首关于Hugging Face社区的俳句，并将其写入我桌面上的"hf.txt"文件

现在让我们尝试这个涉及网页浏览的提示：

> 在Brave Search上搜索HF推理提供商并打开前3个结果

<video controls autoplay loop>
  <source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/tiny-agents/brave-search.mp4" type="video/mp4">
</video>

连接我们的Gradio情感分析工具后，我们可以类似地问：
> 分析这条评论的情感："我绝对喜欢这个产品，它超出了我所有的期望！"

### 默认模型和提供商

在模型/提供商对方面，我们的示例Agent默认使用：
- ["Qwen/Qwen2.5-72B-Instruct"](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)
- 运行在[Nebius](https://huggingface.co/docs/inference-providers/providers/nebius)上

这些都是可以通过环境变量配置的！在这里，我们还将展示如何添加我们的Gradio MCP服务器：

```ts
const agent = new Agent({
	provider: process.env.PROVIDER ?? "nebius",
	model: process.env.MODEL_ID ?? "Qwen/Qwen2.5-72B-Instruct",
	apiKey: process.env.HF_TOKEN,
	servers: [
		// 默认服务器
		{
			command: "npx",
			args: ["@modelcontextprotocol/servers", "filesystem"]
		},
		{
			command: "npx",
			args: ["playwright-mcp"]
		},
		// 我们的Gradio情感分析服务器
		{
			command: "npx",
			args: [
				"mcp-remote",
				"http://localhost:7860/gradio_api/mcp/sse"
			]
		}
	],
});
```

<Tip>

我们通过[`mcp-remote`](https://www.npmjs.com/package/mcp-remote)包连接到我们的基于Gradio的MCP服务器。

</Tip>

## 基础：LLM中的工具调用原生支持

使将Gradio MCP服务器连接到我们的Tiny Agent成为可能的是，最近的LLM（包括闭源和开源的）都经过函数调用（也称为工具使用）的训练。这种能力同样支持我们与使用Gradio构建的情感分析工具的集成。

工具由其名称、描述和参数的JSONSchema表示定义 - 这正是我们在Gradio服务器中定义情感分析函数的方式。让我们看一个简单的例子：

```ts
const weatherTool = {
	type: "function",
	function: {
		name: "get_weather",
		description: "获取给定位置的当前温度。",
		parameters: {
			type: "object",
			properties: {
				location: {
					type: "string",
					description: "城市和国家，例如：波哥大，哥伦比亚",
				},
			},
		},
	},
};
```

我们的Gradio情感分析工具会有类似的结构，只是输入参数是`text`而不是`location`。

我将在这里链接的规范文档是[OpenAI的函数调用文档](https://platform.openai.com/docs/guides/function-calling?api-mode=chat)。（是的...OpenAI几乎为整个社区定义了LLM标准😅）。

推理引擎允许你在调用LLM时传递工具列表，LLM可以自由调用零个、一个或多个这些工具。
作为开发者，你运行这些工具并将它们的结果反馈给LLM以继续生成。

> 注意，在后端（在推理引擎级别），工具只是以特殊格式的`chat_template`传递给模型，就像任何其他消息一样，然后从响应中解析出来（使用模型特定的特殊标记）以将它们作为工具调用暴露出来。

## 在InferenceClient之上实现MCP客户端

现在我们知道最近LLM中的工具是什么，让我们实现实际的MCP客户端，它将与我们的Gradio服务器和其他MCP服务器通信。

https://modelcontextprotocol.io/quickstart/client 上的官方文档写得相当好。你只需要用任何其他OpenAI兼容的客户端SDK替换对Anthropic客户端SDK的任何提及。（还有一个[llms.txt](https://modelcontextprotocol.io/llms-full.txt)你可以输入到你选择的LLM中以帮助你编码）。

提醒一下，我们使用HF的`InferenceClient`作为我们的推理客户端。

> [!TIP]
> 如果你想使用实际代码跟随，完整的`McpClient.ts`代码文件在[这里](https://github.com/huggingface/huggingface.js/blob/main/packages/mcp-client/src/McpClient.ts) 🤓

我们的`McpClient`类有：
- 一个推理客户端（适用于任何推理提供商，`huggingface/inference`支持远程和本地端点）
- 一组MCP客户端会话，每个连接的MCP服务器一个（这允许我们连接到多个服务器，包括我们的Gradio服务器）
- 以及一个可用工具列表，它将从连接的服务器中填充并稍微重新格式化。

```ts
export class McpClient {
	protected client: InferenceClient;
	protected provider: string;
	protected model: string;
	private clients: Map<ToolName, Client> = new Map();
	public readonly availableTools: ChatCompletionInputTool[] = [];

	constructor({ provider, model, apiKey }: { provider: InferenceProvider; model: string; apiKey: string }) {
		this.client = new InferenceClient(apiKey);
		this.provider = provider;
		this.model = model;
	}
	
	// [...]
}
```

要连接到MCP服务器（如我们的Gradio情感分析服务器），官方的`@modelcontextprotocol/sdk/client` TypeScript SDK提供了一个带有`listTools()`方法的`Client`类：

```ts
async addMcpServer(server: StdioServerParameters): Promise<void> {
	const transport = new StdioClientTransport({
		...server,
		env: { ...server.env, PATH: process.env.PATH ?? "" },
	});
	const mcp = new Client({ name: "@huggingface/mcp-client", version: packageVersion });
	await mcp.connect(transport);

	const toolsResult = await mcp.listTools();
	debug(
		"Connected to server with tools:",
		toolsResult.tools.map(({ name }) => name)
	);

	for (const tool of toolsResult.tools) {
		this.clients.set(tool.name, mcp);
	}

	this.availableTools.push(
		...toolsResult.tools.map((tool) => {
			return {
				type: "function",
				function: {
					name: tool.name,
					description: tool.description,
					parameters: tool.inputSchema,
				},
			} satisfies ChatCompletionInputTool;
		})
	);
}
```

`StdioServerParameters`是来自MCP SDK的一个接口，它将让你轻松生成一个本地进程：正如我们前面提到的，目前所有MCP服务器实际上都是本地进程，包括我们的Gradio服务器（尽管我们通过HTTP访问它）。

对于我们连接的每个MCP服务器（包括我们的Gradio情感分析服务器），我们稍微重新格式化其工具列表并将它们添加到`this.availableTools`中。

### 如何使用工具

使用我们的情感分析工具（或任何其他MCP工具）很简单。你只需将`this.availableTools`传递给你的LLM聊天完成，除了你通常的消息数组：

```ts
const stream = this.client.chatCompletionStream({
	provider: this.provider,
	model: this.model,
	messages,
	tools: this.availableTools,
	tool_choice: "auto",
});
```

`tool_choice: "auto"`是你传递的参数，让LLM生成零个、一个或多个工具调用。

在解析或流式传输输出时，LLM将生成一些工具调用（即函数名称和一些JSON编码的参数），你需要（作为开发者）计算这些调用。MCP客户端SDK再次使这变得非常容易；它有一个`client.callTool()`方法：

```ts
const toolName = toolCall.function.name;
const toolArgs = JSON.parse(toolCall.function.arguments);

const toolMessage: ChatCompletionInputMessageTool = {
	role: "tool",
	tool_call_id: toolCall.id,
	content: "",
	name: toolName,
};

/// 获取此工具的适当会话
const client = this.clients.get(toolName);
if (client) {
	const result = await client.callTool({ name: toolName, arguments: toolArgs });
	toolMessage.content = result.content[0].text;
} else {
	toolMessage.content = `Error: No session found for tool: ${toolName}`;
}
```

如果LLM选择使用我们的情感分析工具，这段代码将自动将调用路由到我们的Gradio服务器，执行分析，并将结果返回给LLM。

最后，你将把生成的工具消息添加到你的`messages`数组中并返回给LLM。

## 我们的50行代码Agent 🤯

现在我们有了一个能够连接到任意MCP服务器（包括我们的Gradio情感分析服务器）以获取工具列表的MCP客户端，并且能够将它们注入并从LLM推理中解析出来，那么...什么是Agent？

> 一旦你有了一个带有一组工具的推理客户端，那么Agent就只是它上面的一个while循环。

更详细地说，Agent只是以下内容的组合：
- 一个系统提示
- 一个LLM推理客户端
- 一个MCP客户端，用于从一堆MCP服务器（包括我们的Gradio服务器）中钩入一组工具
- 一些基本的控制流（见下面的while循环）

> [!TIP]
> 完整的`Agent.ts`代码文件在[这里](https://github.com/huggingface/huggingface.js/blob/main/packages/mcp-client/src/Agent.ts)。

我们的Agent类简单地扩展了McpClient：

```ts
export class Agent extends McpClient {
	private readonly servers: StdioServerParameters[];
	protected messages: ChatCompletionInputMessage[];

	constructor({
		provider,
		model,
		apiKey,
		servers,
		prompt,
	}: {
		provider: InferenceProvider;
		model: string;
		apiKey: string;
		servers: StdioServerParameters[];
		prompt?: string;
	}) {
		super({ provider, model, apiKey });
		this.servers = servers;
		this.messages = [
			{
				role: "system",
				content: prompt ?? DEFAULT_SYSTEM_PROMPT,
			},
		];
	}
}
```

默认情况下，我们使用一个非常简单的系统提示，灵感来自[GPT-4.1提示指南](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)中分享的提示。

尽管这来自OpenAI 😈，但这句话特别适用于越来越多的模型，包括闭源和开源的：

> 我们鼓励开发者专门使用tools字段来传递工具，而不是像过去一些人报告的那样，手动将工具描述注入到你的提示中并编写一个单独的工具调用解析器。

也就是说，我们不需要在提示中提供精心格式化的工具使用示例列表。`tools: this.availableTools`参数就足够了，LLM将知道如何使用文件系统工具和我们的Gradio情感分析工具。

在Agent上加载工具实际上只是连接到我们想要的MCP服务器（并行进行，因为在JS中很容易做到）：

```ts
async loadTools(): Promise<void> {
	await Promise.all(this.servers.map((s) => this.addMcpServer(s)));
}
```

我们添加了两个额外的工具（在MCP之外），LLM可以使用它们来控制我们的Agent的流程：

```ts
const taskCompletionTool: ChatCompletionInputTool = {
	type: "function",
	function: {
		name: "task_complete",
		description: "当用户给定的任务完成时调用此工具",
		parameters: {
			type: "object",
			properties: {},
		},
	},
};
const askQuestionTool: ChatCompletionInputTool = {
	type: "function",
	function: {
		name: "ask_question",
		description: "向用户提问以获取解决或澄清他们问题所需的更多信息。",
		parameters: {
			type: "object",
			properties: {},
		},
	},
};
const exitLoopTools = [taskCompletionTool, askQuestionTool];
```

当调用这些工具中的任何一个时，Agent将打破其循环并将控制权交还给用户以获取新的输入。

### 完整的while循环

看我们的完整while循环。🎉

我们的Agent主while循环的要点是，我们只需与LLM迭代，在工具调用和向其提供工具结果之间交替，我们这样做**直到LLM开始连续响应两个非工具消息**。

这是完整的while循环：

```ts
let numOfTurns = 0;
let nextTurnShouldCallTools = true;
while (true) {
	try {
		yield* this.processSingleTurnWithTools(this.messages, {
			exitLoopTools,
			exitIfFirstChunkNoTool: numOfTurns > 0 && nextTurnShouldCallTools,
			abortSignal: opts.abortSignal,
		});
	} catch (err) {
		if (err instanceof Error && err.message === "AbortError") {
			return;
		}
		throw err;
	}
	numOfTurns++;
	const currentLast = this.messages.at(-1)!;
	if (
		currentLast.role === "tool" &&
		currentLast.name &&
		exitLoopTools.map((t) => t.function.name).includes(currentLast.name)
	) {
		return;
	}
	if (currentLast.role !== "tool" && numOfTurns > MAX_NUM_TURNS) {
		return;
	}
	if (currentLast.role !== "tool" && nextTurnShouldCallTools) {
		return;
	}
	if (currentLast.role === "tool") {
		nextTurnShouldCallTools = false;
	} else {
		nextTurnShouldCallTools = true;
	}
}
```

## 将Tiny Agents与Gradio MCP服务器连接

现在我们已经理解了Tiny Agents和Gradio MCP服务器，让我们看看它们如何协同工作！MCP的美妙之处在于它提供了一种标准化的方式，让智能体可以与任何MCP兼容的服务器交互，包括我们基于Gradio的情感分析服务器。

### 将Tiny Agents与Gradio服务器一起使用

要将我们的Tiny Agent连接到我们之前构建的Gradio情感分析服务器，我们只需要将其添加到我们的服务器列表中。以下是我们如何修改我们的agent配置：

```ts
const agent = new Agent({
    provider: process.env.PROVIDER ?? "nebius",
    model: process.env.MODEL_ID ?? "Qwen/Qwen2.5-72B-Instruct",
    apiKey: process.env.HF_TOKEN,
    servers: [
        // ... 现有服务器 ...
        {
            command: "npx",
            args: [
                "mcp-remote",
                "http://localhost:7860/gradio_api/mcp/sse"  // 你的Gradio MCP服务器
            ]
        }
    ],
});
```

现在我们的agent可以与其他工具一起使用情感分析工具！例如，它可以：
1. 使用文件系统服务器从文件中读取文本
2. 使用我们的Gradio服务器分析其情感
3. 将结果写回文件

### 示例交互

以下是与我们的agent的对话可能的样子：

```
用户：从我桌面读取文件"feedback.txt"并分析其情感

Agent：我将帮助你分析反馈文件的情感。让我分步骤进行：

1. 首先，我将使用文件系统工具读取文件
2. 然后，我将使用情感分析工具分析其情感
3. 最后，我将把结果写入新文件

[Agent继续使用工具并提供分析]
```

### 部署注意事项

当你将Gradio MCP服务器部署到Hugging Face Spaces时，你需要更新agent配置中的服务器URL，指向你部署的空间：

```ts
{
    command: "npx",
    args: [
        "mcp-remote",
        "https://YOUR_USERNAME-mcp-sentiment.hf.space/gradio_api/mcp/sse"
    ]
}
```

这允许你的agent从任何地方使用情感分析工具，而不仅仅是在本地！



