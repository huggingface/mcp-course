# Tiny Agents: um agente MCP em 50 linhas de c√≥digo

Agora que constru√≠mos servidores MCP no Gradio e aprendemos sobre a cria√ß√£o de clientes MCP, vamos completar nossa aplica√ß√£o completa construindo um agente TypeScript que pode interagir perfeitamente com nossa ferramenta de an√°lise de sentimentos. Esta se√ß√£o √© baseada no projeto [Tiny Agents](https://huggingface.co/blog/tiny-agents), que demonstra uma maneira super simples de implantar clientes MCP que podem se conectar a servi√ßos como nosso servidor de an√°lise de sentimentos Gradio.

Neste exerc√≠cio final da Unidade 2, vamos orient√°-lo na implementa√ß√£o de um cliente MCP TypeScript (JS) que pode se comunicar com qualquer servidor MCP, incluindo o servidor de an√°lise de sentimentos baseado em Gradio que constru√≠mos nas se√ß√µes anteriores. Isso completa nosso fluxo de aplica√ß√£o MCP completo: desde a constru√ß√£o de um servidor MCP Gradio expondo uma ferramenta de an√°lise de sentimentos, at√© a cria√ß√£o de um agente flex√≠vel que pode usar esta ferramenta junto com outras capacidades.

![meme](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/tiny-agents/thumbnail.jpg)
<figcaption>Cr√©dito da imagem https://x.com/adamdotdev</figcaption>

## Instala√ß√£o

Primeiro, precisamos instalar o pacote `tiny-agents`.

```bash
npm install @huggingface/tiny-agents
# ou
pnpm add @huggingface/tiny-agents
```

Em seguida, precisamos instalar o pacote `mcp-remote`.

```bash
npm install @mcpjs/mcp-remote
# ou
pnpm add @mcpjs/mcp-remote
```

## Cliente MCP Tiny Agents na Linha de Comando

Tiny Agents pode criar clientes MCP a partir da linha de comando com base em arquivos de configura√ß√£o JSON.

Vamos configurar um projeto com um Tiny Agent b√°sico.

```bash
mkdir my-agent
touch my-agent/agent.json
```

O arquivo JSON ficar√° assim:

```json
{
	"model": "Qwen/Qwen2.5-72B-Instruct",
	"provider": "nebius",
	"servers": [
		{
			"type": "stdio",
			"config": {
				"command": "npx",
				"args": [
					"mcp-remote",
					"http://localhost:7860/gradio_api/mcp/sse"
				]
			}
		}
	]
}
```

Aqui temos um Tiny Agent b√°sico que pode se conectar ao nosso servidor MCP Gradio. Ele inclui um modelo, provedor e uma configura√ß√£o de servidor.

| Campo | Descri√ß√£o |
|-------|-------------|
| `model` | O modelo de c√≥digo aberto a ser usado pelo agente |
| `provider` | O provedor de infer√™ncia a ser usado pelo agente |
| `servers` | Os servidores a serem usados pelo agente. Usaremos o servidor `mcp-remote` para nosso servidor MCP Gradio. |

Tamb√©m poder√≠amos usar um modelo de c√≥digo aberto executando localmente com Tiny Agents.

```json
{
	"model": "Qwen/Qwen3-32B",
	"endpointUrl": "http://localhost:1234/v1",
	"servers": [
		{
			"type": "stdio",
			"config": {
				"command": "npx",
				"args": [
					"mcp-remote",
					"http://localhost:1234/v1/mcp/sse"
				]
			}
		}
	]
}
```

Aqui temos um Tiny Agent que pode se conectar a um modelo local. Ele inclui um modelo, URL de endpoint (`http://localhost:1234/v1`) e uma configura√ß√£o de servidor. O endpoint deve ser um endpoint compat√≠vel com OpenAI.

Podemos ent√£o executar o agente com o seguinte comando:

```bash
npx @huggingface/tiny-agents run ./my-agent
```

## Cliente MCP Tiny Agents Personalizado

Agora que entendemos tanto os Tiny Agents quanto os servidores MCP Gradio, vamos ver como eles funcionam juntos! A beleza do MCP √© que ele fornece uma maneira padronizada para os agentes interagirem com qualquer servidor compat√≠vel com MCP, incluindo nosso servidor de an√°lise de sentimentos baseado em Gradio das se√ß√µes anteriores.

### Usando o Servidor Gradio com Tiny Agents

Para conectar nosso Tiny Agent ao servidor de an√°lise de sentimentos Gradio que constru√≠mos anteriormente nesta unidade, s√≥ precisamos adicion√°-lo √† nossa lista de servidores. Veja como podemos modificar nossa configura√ß√£o de agente:

```ts
const agent = new Agent({
    provider: process.env.PROVIDER ?? "nebius",
    model: process.env.MODEL_ID ?? "Qwen/Qwen2.5-72B-Instruct",
    apiKey: process.env.HF_TOKEN,
    servers: [
        // ... servidores existentes ...
        {
            command: "npx",
            args: [
                "mcp-remote",
                "http://localhost:7860/gradio_api/mcp/sse"  // Seu servidor MCP Gradio
            ]
        }
    ],
});
```

Agora nosso agente pode usar a ferramenta de an√°lise de sentimentos junto com outras ferramentas! Por exemplo, ele poderia:
1. Ler texto de um arquivo usando o servidor de sistema de arquivos
2. Analisar seu sentimento usando nosso servidor Gradio
3. Escrever os resultados de volta em um arquivo

### Exemplo de Intera√ß√£o

Veja como seria uma conversa com nosso agente:

```
Usu√°rio: Leia o arquivo "feedback.txt" do meu Desktop e analise seu sentimento

Agente: Vou ajud√°-lo a analisar o sentimento do arquivo de feedback. Vou dividir isso em etapas:

1. Primeiro, vou ler o arquivo usando a ferramenta do sistema de arquivos
2. Em seguida, vou analisar seu sentimento usando a ferramenta de an√°lise de sentimentos
3. Por fim, vou escrever os resultados em um novo arquivo

[O Agente prossegue usando as ferramentas e fornecendo a an√°lise]
```

### Considera√ß√µes de Implanta√ß√£o

Ao implantar seu servidor MCP Gradio no Hugging Face Spaces, voc√™ precisar√° atualizar a URL do servidor em sua configura√ß√£o de agente para apontar para seu espa√ßo implantado:

```ts
{
    command: "npx",
    args: [
        "mcp-remote",
        "https://YOUR_USERNAME-mcp-sentiment.hf.space/gradio_api/mcp/sse"
    ]
}
```

Isso permite que seu agente use a ferramenta de an√°lise de sentimentos de qualquer lugar, n√£o apenas localmente!

## Conclus√£o: Nossa Aplica√ß√£o MCP Completa

Nesta unidade, passamos do entendimento dos conceitos b√°sicos do MCP para a constru√ß√£o de uma aplica√ß√£o completa:

1. Criamos um servidor MCP Gradio que exp√µe uma ferramenta de an√°lise de sentimentos
2. Aprendemos como se conectar a este servidor usando clientes MCP
3. Constru√≠mos um tiny agent em TypeScript que pode interagir com nossa ferramenta

Isso demonstra o poder do Protocolo de Contexto de Modelo - podemos criar ferramentas especializadas usando frameworks com os quais estamos familiarizados (como Gradio), exp√¥-las atrav√©s de uma interface padronizada (MCP) e, em seguida, ter agentes usando essas ferramentas perfeitamente junto com outras capacidades.

O fluxo completo que constru√≠mos permite que um agente:
- Se conecte a m√∫ltiplos provedores de ferramentas
- Descubra dinamicamente as ferramentas dispon√≠veis
- Use nossa ferramenta personalizada de an√°lise de sentimentos
- Combine-a com outras capacidades como acesso ao sistema de arquivos e navega√ß√£o na web

Esta abordagem modular √© o que torna o MCP t√£o poderoso para construir aplica√ß√µes de IA flex√≠veis.

## B√¥nus: Construir um Servidor MCP de Automa√ß√£o de Navegador com Playwright

Como b√¥nus, vamos explorar como usar o servidor MCP Playwright para automa√ß√£o de navegador com Tiny Agents. Isso demonstra a extensibilidade do ecossistema MCP al√©m do nosso exemplo de an√°lise de sentimentos.

<Tip>

Esta se√ß√£o √© baseada no [post do blog Tiny Agents](https://huggingface.co/blog/tiny-agents) e adaptada para o curso MCP.

</Tip>

Nesta se√ß√£o, mostraremos como construir um agente que pode realizar tarefas de automa√ß√£o web como pesquisar, clicar e extrair informa√ß√µes de sites.

```ts
// playwright-agent.ts
import { Agent } from "@huggingface/tiny-agents";

const agent = new Agent({
  provider: process.env.PROVIDER ?? "nebius",
  model: process.env.MODEL_ID ?? "Qwen/Qwen2.5-72B-Instruct",
  apiKey: process.env.HF_TOKEN,
  servers: [
    {
      command: "npx",
      args: ["playwright-mcp"]
    }
  ],
});

await agent.run();
```

O servidor MCP Playwright exp√µe ferramentas que permitem que seu agente:

1. Abra abas do navegador
2. Navegue para URLs
3. Clique em elementos
4. Digite em formul√°rios
5. Extraia conte√∫do de p√°ginas web
6. Tire capturas de tela

Veja um exemplo de intera√ß√£o com nosso agente de automa√ß√£o de navegador:

```
Usu√°rio: Pesquise por "tiny agents" no GitHub e colete os nomes dos 3 principais reposit√≥rios

Agente: Vou pesquisar reposit√≥rios de "tiny agents" no GitHub.
[O Agente abre o navegador, navega at√© o GitHub, realiza a pesquisa e extrai os nomes dos reposit√≥rios]

Aqui est√£o os 3 principais reposit√≥rios (n√£o reais) para "tiny agents":
1. huggingface/tiny-agents
2. modelcontextprotocol/tiny-agents-examples
3. langchain/tiny-agents-js
```

Esta capacidade de automa√ß√£o de navegador pode ser combinada com outros servidores MCP para criar fluxos de trabalho poderosos‚Äîpor exemplo, extraindo texto de uma p√°gina web e depois analisando-o com ferramentas personalizadas.

## Como executar a demonstra√ß√£o completa

Se voc√™ tem NodeJS (com `pnpm` ou `npm`), basta executar isto em um terminal:

```bash
npx @huggingface/mcp-client
```

ou se estiver usando `pnpm`:

```bash
pnpx @huggingface/mcp-client
```

Isso instala o pacote em uma pasta tempor√°ria e depois executa seu comando.

Voc√™ ver√° seu Agente simples se conectar a m√∫ltiplos servidores MCP (executando localmente), carregando suas ferramentas (semelhante a como carregaria sua ferramenta de an√°lise de sentimentos Gradio), e depois solicitando uma conversa.

<video controls autoplay loop>
  <source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/tiny-agents/use-filesystem.mp4" type="video/mp4">
</video>

Por padr√£o, nosso Agente de exemplo se conecta aos seguintes dois servidores MCP:

- o servidor de sistema de arquivos [can√¥nico](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem), que obt√©m acesso ao seu Desktop,
- e o servidor [Playwright MCP](https://github.com/microsoft/playwright-mcp), que sabe como usar um navegador Chromium sandbox para voc√™.

> [!NOTE]
> Nota: isso √© um pouco contraintuitivo, mas atualmente, todos os servidores MCP no tiny agents s√£o processos locais (embora servidores remotos estejam chegando em breve).

Nossa entrada para este primeiro v√≠deo foi:

> escreva um haiku sobre a comunidade Hugging Face e escreva-o em um arquivo chamado "hf.txt" no meu Desktop

Agora vamos tentar este prompt que envolve navega√ß√£o na Web:

> fa√ßa uma Pesquisa Web por provedores de infer√™ncia HF no Brave Search e abra os 3 primeiros resultados

<video controls autoplay loop>
  <source src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/tiny-agents/brave-search.mp4" type="video/mp4">
</video>

### Modelo e provedor padr√£o

Em termos de par modelo/provedor, nosso Agente de exemplo usa por padr√£o:
- ["Qwen/Qwen2.5-72B-Instruct"](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)
- executando no [Nebius](https://huggingface.co/docs/inference-providers/providers/nebius)

Tudo isso √© configur√°vel atrav√©s de vari√°veis de ambiente:

```ts
const agent = new Agent({
	provider: process.env.PROVIDER ?? "nebius",
	model: process.env.MODEL_ID ?? "Qwen/Qwen2.5-72B-Instruct",
	apiKey: process.env.HF_TOKEN,
	servers: [
		// Servidores padr√£o
		{
			command: "npx",
			args: ["@modelcontextprotocol/servers", "filesystem"]
		},
		{
			command: "npx",
			args: ["playwright-mcp"]
		},
	],
});
```

## Implementando um cliente MCP em cima do InferenceClient

Agora que sabemos o que √© uma ferramenta em LLMs recentes, vamos implementar o cliente MCP real que se comunicar√° com servidores MCP e outros servidores MCP.

A documenta√ß√£o oficial em https://modelcontextprotocol.io/quickstart/client √© bastante bem escrita. Voc√™ s√≥ precisa substituir qualquer men√ß√£o ao SDK do cliente Anthropic por qualquer outro SDK de cliente compat√≠vel com OpenAI. (H√° tamb√©m um [llms.txt](https://modelcontextprotocol.io/llms-full.txt) que voc√™ pode alimentar no seu LLM de escolha para ajud√°-lo a codificar).

Como lembrete, usamos o `InferenceClient` do HF para nosso cliente de infer√™ncia.

> [!TIP]
> O arquivo de c√≥digo completo `McpClient.ts` est√° [aqui](https://github.com/huggingface/huggingface.js/blob/main/packages/mcp-client/src/McpClient.ts) se voc√™ quiser acompanhar usando o c√≥digo real ü§ì

Nossa classe `McpClient` tem:
- um Cliente de Infer√™ncia (funciona com qualquer Provedor de Infer√™ncia, e `huggingface/inference` suporta endpoints remotos e locais)
- um conjunto de sess√µes de cliente MCP, uma para cada servidor MCP conectado (isso nos permite conectar a m√∫ltiplos servidores)
- e uma lista de ferramentas dispon√≠veis que ser√° preenchida a partir dos servidores conectados e apenas ligeiramente reformatada.

```ts
export class McpClient {
	protected client: InferenceClient;
	protected provider: string;
	protected model: string;
	private clients: Map<ToolName, Client> = new Map();
	public readonly availableTools: ChatCompletionInputTool[] = [];

	constructor({ provider, model, apiKey }: { provider: InferenceProvider; model: string; apiKey: string }) {
		this.client = new InferenceClient(apiKey);
		this.provider = provider;
		this.model = model;
	}
	
	// [...]
}
```

Para se conectar a um servidor MCP, o SDK TypeScript oficial `@modelcontextprotocol/sdk/client` fornece uma classe `Client` com um m√©todo `listTools()`:

```ts
async addMcpServer(server: StdioServerParameters): Promise<void> {
	const transport = new StdioClientTransport({
		...server,
		env: { ...server.env, PATH: process.env.PATH ?? "" },
	});
	const mcp = new Client({ name: "@huggingface/mcp-client", version: packageVersion });
	await mcp.connect(transport);

	const toolsResult = await mcp.listTools();
	debug(
		"Connected to server with tools:",
		toolsResult.tools.map(({ name }) => name)
	);

	for (const tool of toolsResult.tools) {
		this.clients.set(tool.name, mcp);
	}

	this.availableTools.push(
		...toolsResult.tools.map((tool) => {
			return {
				type: "function",
				function: {
					name: tool.name,
					description: tool.description,
					parameters: tool.inputSchema,
				},
			} satisfies ChatCompletionInputTool;
		})
	);
}
```

`StdioServerParameters` √© uma interface do SDK MCP que permitir√° que voc√™ facilmente inicie um processo local: como mencionamos anteriormente, atualmente, todos os servidores MCP s√£o processos locais.

### Como usar as ferramentas

Usar nossa ferramenta de an√°lise de sentimentos (ou qualquer outra ferramenta MCP) √© simples. Voc√™ apenas passa `this.availableTools` para a conclus√£o de chat do seu LLM, al√©m do seu array habitual de mensagens:

```ts
const stream = this.client.chatCompletionStream({
	provider: this.provider,
	model: this.model,
	messages,
	tools: this.availableTools,
	tool_choice: "auto",
});
```

`tool_choice: "auto"` √© o par√¢metro que voc√™ passa para o LLM gerar zero, uma ou m√∫ltiplas chamadas de ferramentas.

Ao analisar ou transmitir a sa√≠da, o LLM gerar√° algumas chamadas de ferramentas (ou seja, um nome de fun√ß√£o e alguns argumentos codificados em JSON), que voc√™ (como desenvolvedor) precisa computar. O SDK cliente MCP novamente facilita isso; ele tem um m√©todo `client.callTool()`:

```ts
const toolName = toolCall.function.name;
const toolArgs = JSON.parse(toolCall.function.arguments);

const toolMessage: ChatCompletionInputMessageTool = {
	role: "tool",
	tool_call_id: toolCall.id,
	content: "",
	name: toolName,
};

/// Obter a sess√£o apropriada para esta ferramenta
const client = this.clients.get(toolName);
if (client) {
	const result = await client.callTool({ name: toolName, arguments: toolArgs });
	toolMessage.content = result.content[0].text;
} else {
	toolMessage.content = `Error: No session found for tool: ${toolName}`;
}
```

Se o LLM escolher usar uma ferramenta, este c√≥digo automaticamente encaminhar√° a chamada para o servidor MCP, executar√° a an√°lise e retornar√° o resultado ao LLM.

Finalmente, voc√™ adicionar√° a mensagem de ferramenta resultante ao seu array `messages` e de volta ao LLM.

## Nosso Agente de 50 linhas de c√≥digo ü§Ø

Agora que temos um cliente MCP capaz de se conectar a servidores MCP arbitr√°rios para obter listas de ferramentas e capaz de injet√°-las e analis√°-las da infer√™ncia do LLM, bem... o que √© um Agente?

> Uma vez que voc√™ tem um cliente de infer√™ncia com um conjunto de ferramentas, ent√£o um Agente √© apenas um loop while em cima dele.

Em mais detalhes, um Agente √© simplesmente uma combina√ß√£o de:
- um prompt de sistema
- um cliente de infer√™ncia LLM
- um cliente MCP para conectar um conjunto de Ferramentas a partir de v√°rios servidores MCP
- algum fluxo de controle b√°sico (veja abaixo para o loop while)

> [!TIP]
> O arquivo de c√≥digo completo `Agent.ts` est√° [aqui](https://github.com/huggingface/huggingface.js/blob/main/packages/mcp-client/src/Agent.ts).

Nossa classe Agent simplesmente estende McpClient:

```ts
export class Agent extends McpClient {
	private readonly servers: StdioServerParameters[];
	protected messages: ChatCompletionInputMessage[];

	constructor({
		provider,
		model,
		apiKey,
		servers,
		prompt,
	}: {
		provider: InferenceProvider;
		model: string;
		apiKey: string;
		servers: StdioServerParameters[];
		prompt?: string;
	}) {
		super({ provider, model, apiKey });
		this.servers = servers;
		this.messages = [
			{
				role: "system",
				content: prompt ?? DEFAULT_SYSTEM_PROMPT,
			},
		];
	}
}
```

Por padr√£o, usamos um prompt de sistema muito simples inspirado no compartilhado no [guia de prompting do GPT-4.1](https://cookbook.openai.com/examples/gpt4-1_prompting_guide).

Mesmo que isso venha da OpenAI üòà, esta frase em particular se aplica a cada vez mais modelos, tanto fechados quanto abertos:

> Incentivamos os desenvolvedores a usar exclusivamente o campo tools para passar ferramentas, em vez de injetar manualmente descri√ß√µes de ferramentas em seu prompt e escrever um analisador separado para chamadas de ferramentas, como alguns relataram fazer no passado.

O que quer dizer, n√£o precisamos fornecer listas meticulosamente formatadas de exemplos de uso de ferramentas no prompt. O par√¢metro `tools: this.availableTools` √© suficiente, e o LLM saber√° como usar as ferramentas do sistema de arquivos.

Carregar as ferramentas no Agente √© literalmente apenas conectar-se aos servidores MCP que queremos (em paralelo porque √© t√£o f√°cil de fazer em JS):

```ts
async loadTools(): Promise<void> {
	await Promise.all(this.servers.map((s) => this.addMcpServer(s)));
}
```

Adicionamos duas ferramentas extras (fora do MCP) que podem ser usadas pelo LLM para o fluxo de controle do nosso Agente:

```ts
const taskCompletionTool: ChatCompletionInputTool = {
	type: "function",
	function: {
		name: "task_complete",
		description: "Call this tool when the task given by the user is complete",
		parameters: {
			type: "object",
			properties: {},
		},
	},
};
const askQuestionTool: ChatCompletionInputTool = {
	type: "function",
	function: {
		name: "ask_question",
		description: "Ask a question to the user to get more info required to solve or clarify their problem.",
		parameters: {
			type: "object",
			properties: {},
		},
	},
};
const exitLoopTools = [taskCompletionTool, askQuestionTool];
```

Ao chamar qualquer uma dessas ferramentas, o Agente quebrar√° seu loop e devolver√° o controle ao usu√°rio para nova entrada.

### O loop while completo

Contemplem nosso loop while completo.üéâ

A ess√™ncia do loop principal do nosso Agente √© que simplesmente iteramos com o LLM alternando entre chamadas de ferramentas e alimentando-o com os resultados das ferramentas, e fazemos isso **at√© que o LLM comece a responder com duas mensagens n√£o-ferramentas consecutivas**.

Este √© o loop while completo:

```ts
let numOfTurns = 0;
let nextTurnShouldCallTools = true;
while (true) {
	try {
		yield* this.processSingleTurnWithTools(this.messages, {
			exitLoopTools,
			exitIfFirstChunkNoTool: numOfTurns > 0 && nextTurnShouldCallTools,
			abortSignal: opts.abortSignal,
		});
	} catch (err) {
		if (err instanceof Error && err.message === "AbortError") {
			return;
		}
		throw err;
	}
	numOfTurns++;
	const currentLast = this.messages.at(-1)!;
	if (
		currentLast.role === "tool" &&
		currentLast.name &&
		exitLoopTools.map((t) => t.function.name).includes(currentLast.name)
	) {
		return;
	}
	if (currentLast.role !== "tool" && numOfTurns > MAX_NUM_TURNS) {
		return;
	}
	if (currentLast.role !== "tool" && nextTurnShouldCallTools) {
		return;
	}
	if (currentLast.role === "tool") {
		nextTurnShouldCallTools = false;
	} else {
		nextTurnShouldCallTools = true;
	}
}
```
